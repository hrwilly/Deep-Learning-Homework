{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "PyTorch tensors work like numpy tensors (arrays) and can for the most part be used like them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1, 2], [3, 4]]\n",
    "xt = torch.tensor(x)\n",
    "xn = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xt)\n",
    "print(xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place operations\n",
    "\n",
    "PyTorch has a number of operations that changes the contents of a tensor. Similarly to in Julia, these are by convention by an underscore at the end of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor, \"\\n\")\n",
    "\n",
    "tensor.add(5) # This is the same as writing \"tensor + 5\"\n",
    "print(tensor, \"\\n\") # Therefore, the tensor is unchanged\n",
    "\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors can be located on either the CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors (on the CPU) can often be used in place of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.lstsq(xt, np.ones(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually make a numpy array, `.numpy()` extracts a numpy array from a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tensors are using the same underlying data, which means that changes to one are reflected in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the tensor\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the numpy array\n",
    "n[0] *= 10\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: When we use gradients, the gradients can under various circumstances be wrong if the tensor is not detached first using `.detach()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a GPU\n",
    "\n",
    "My main device is from NVIDIA, so we here use the CUDA module.\n",
    "\n",
    "Existing CPU data can be moved to a GPU using `.to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    xt = xt.to('cuda')\n",
    "    print(f\"Device tensor is stored on: {xt.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multiple GPUs, you specify which one to use. This lists them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    if i == torch.cuda.current_device():\n",
    "        print(\"-> \", end=\"\")\n",
    "    else:\n",
    "        print(\"   \", end=\"\")\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set PyTorch to use the CUDA enabled GPU by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other devices, such as Apple's chips, calls have to be modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "    torch.set_default_device('mps')\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.tensor([[1, 2], [3, 4]])\n",
    "xt.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be moved to and from the GPU if needed, for instance when you want to process the data using a non-PyTorch system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.lstsq(xt, np.ones(2)) # This does not work because GPU tensors don't work with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.lstsq(xt.cpu(), np.ones(2)) # Moving it to the CPU first works\n",
    "xt.cpu().device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not move data between the GPU and CPU unnecessarily, as this is a relatively slow operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "We will use simple linear models here as an example. The goal is to train the least squares objective by gradient descent. In other words,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_\\beta \\| y - \\beta X \\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "and we need to take gradients in $\\beta$.\n",
    "\n",
    "PyTorch can compute gradients for us. To tell PyTorch that we want gradients for a particular tensor, we add `requires_grad=True` when instantiating, like for `beta` below. After this, *every* computation involving beta will be tracked for gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is not ideal for GPU computation...\n",
    "# The .cpu() calls can be removed but are kept for generality.\n",
    "torch.set_default_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.zeros(2, requires_grad=True)\n",
    "x = 42 + 2. * torch.arange(30) # Note the floating point 2. as opposed to just 2\n",
    "y = 130 + 0.6 * x + 5 * torch.rand_like(x)\n",
    "X = torch.column_stack((torch.ones_like(y), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running computations with `beta` to compute the loss value `loss`, the gradient of `loss` with respect to `beta` can be computed using `loss.backward()`. This stores the gradient inside `beta.grad`, which we use to update `beta`.\n",
    "\n",
    "WARNING: This code is doing it manually for illustration purposes. Do not do it like this!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step_(beta, eps):\n",
    "    loss = torch.mean((y - X @ beta)**2)\n",
    "    loss.backward()\n",
    "    beta.data -= eps * beta.grad.data\n",
    "    beta.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100000):\n",
    "    gradient_step_(beta, 0.0001)\n",
    "print(beta.detach())\n",
    "plt.scatter(x.cpu(), y.cpu())\n",
    "plt.plot(x.cpu(), (X @ beta.detach()).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually updating the parameters, you should be using an optimizer. `torch.optim.SGD` is what we implemented above. In this example we use `Adam`, which is much easier to use than plain gradient descent (try both and see how they change with the learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam((beta,), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    return X @ beta\n",
    "def objective(y, yhat):\n",
    "    return torch.mean((y - yhat)**2)\n",
    "\n",
    "for _ in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    objective(y, model(X)).backward()\n",
    "    optimizer.step()\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network modules\n",
    "\n",
    "PyTorch has build in support for simplifying the transformations. For our example here, it means we do not have to construct `beta` manually and do not have to write the matrix multiplication manually.\n",
    "\n",
    "Here we are interested in a `Linear` module/layer. Its two main arguments are the input dimension and output dimension. The input dimension is the number of features in our input and the output dimension is here 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.nn.Linear(2, 1, bias = False)\n",
    "torch.equal(beta(X), X @ beta.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `bias=True` (which is the default), we do not even need the constant column of `X`. However, note that PyTorch expects inputs to be $N \\times p$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:,torch.newaxis]\n",
    "beta = torch.nn.Linear(1,1)\n",
    "torch.equal(beta(x), beta.bias + x @ beta.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(beta.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "objective(y, beta(x)).backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(beta.bias.data)\n",
    "print(beta.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.nn.Linear(4, 3)\n",
    "print(beta.bias)\n",
    "print(beta.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Pytorch follows python design principles, and the typcal way of constructing a neural network is by subclassing `torch.nn.Module` and defining a `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # These define fully connected transformations\n",
    "        # x -> Wx + b\n",
    "        # Note that the output dimension of each layer\n",
    "        # must match the input of the next.\n",
    "        self.fc1 = nn.Linear(10, 11)\n",
    "        self.fc2 = nn.Linear(11, 12)\n",
    "        self.fc3 = nn.Linear(12, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defining forward is like defining __call__ for\n",
    "        # regular python classes, but specialized for pytorch.\n",
    "        # __call__ should not be overwritten, as it takes care\n",
    "        # of running hooks.\n",
    "\n",
    "        # This gradually runs each layer, with relu activation.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # The size of the first layer weights (not bias) fc1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network can be evaluated by calling it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(torch.rand(4, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
